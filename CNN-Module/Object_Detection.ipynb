{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 23:29:15.091178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-20 23:29:15.126171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-20 23:29:15.136297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-20 23:29:15.310051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json as js\n",
    "import tensorflow as tf\n",
    "from os import listdir as ld\n",
    "from os import path\n",
    "from keras.models import Model #type: ignore\n",
    "from keras import layers,models #type: ignore\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical #type: ignore\n",
    "from keras.losses import Huber #type: ignore\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_CNN.json\",'r') as file:\n",
    "    paths = js.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Annotations: 100%|██████████| 2580003/2580003 [00:03<00:00, 797783.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Validation Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Annotations: 100%|██████████| 110343/110343 [00:00<00:00, 572390.53it/s]\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745171974.463270    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171974.850310    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171974.852625    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171974.857613    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171974.860989    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171974.863142    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171975.054837    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171975.058408    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745171975.060329    5377 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-20 23:29:35.062378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3650 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Paths and constants\n",
    "train_img_dir = paths['Train_resized']\n",
    "train_annotation = paths['Preprocessed_Train_json']\n",
    "val_img_dir = paths['Validation_resized']\n",
    "val_annotation = paths['Preprocessed_Validation_json']\n",
    "NUM_CLASSES = 80\n",
    "\n",
    "# Load annotations\n",
    "with open(train_annotation, 'r') as f:\n",
    "    train_annotations = js.load(f)\n",
    "\n",
    "with open(val_annotation, 'r') as f:\n",
    "    val_annotations = js.load(f)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "print(\"Processing Training Data\")\n",
    "\n",
    "for info in tqdm(train_annotations, desc=\"Training Annotations\"):\n",
    "    bbox = info['bbox']\n",
    "    label = info['category_id']\n",
    "    image_path = path.join(train_img_dir, info[\"img_id\"])\n",
    "    train_data.append((image_path, bbox, label))\n",
    "\n",
    "print(\"Processing Validation Data\")\n",
    "\n",
    "for info in tqdm(val_annotations, desc=\"Validation Annotations\"):\n",
    "    bbox = info['bbox']\n",
    "    label = info['category_id']\n",
    "    image_path = path.join(val_img_dir, info[\"img_id\"])\n",
    "    val_data.append((image_path, bbox, label))\n",
    "\n",
    "def preprocess_example(image_path, bbox, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [64, 64])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    img_shape = tf.shape(image)\n",
    "    img_h = tf.cast(img_shape[0], tf.float32)\n",
    "    img_w = tf.cast(img_shape[1], tf.float32)\n",
    "\n",
    "    # Convert bbox to tensor if not already\n",
    "    bbox = tf.convert_to_tensor(bbox, dtype=tf.float32)\n",
    "\n",
    "    x = bbox[0] / img_w\n",
    "    y = bbox[1] / img_h\n",
    "    w = bbox[2] / img_w\n",
    "    h = bbox[3] / img_h\n",
    "\n",
    "    bbox_tensor = tf.stack([x, y, w, h])\n",
    "    class_tensor = tf.one_hot(label, depth=NUM_CLASSES)\n",
    "\n",
    "    return image, {\n",
    "        \"class_output\": class_tensor,\n",
    "        \"bbox_output\": bbox_tensor\n",
    "    }\n",
    "\n",
    "\n",
    "def create_dataset(data, batch_size=4, shuffle=True, show_progress=False):\n",
    "    if show_progress:\n",
    "        print(\"Preprocessing dataset with progress bar...\")\n",
    "        processed = []\n",
    "        for item in tqdm(data, desc=\"Preprocessing Samples\"):\n",
    "            processed.append(preprocess_example(*item))\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(processed)\n",
    "    else:\n",
    "        paths, bboxes, labels = zip(*data)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((list(paths), list(bboxes), list(labels)))\n",
    "        dataset = dataset.map(lambda p, b, l: preprocess_example(p, b, l), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(data))\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = create_dataset(train_data, batch_size=2, show_progress=False)\n",
    "val_dataset = create_dataset(val_data, batch_size=2, shuffle=False, show_progress=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_tf(box1, box2):\n",
    "    box1_x1 = box1[:, 0]\n",
    "    box1_y1 = box1[:, 1]\n",
    "    box1_x2 = box1[:, 0] + box1[:, 2]\n",
    "    box1_y2 = box1[:, 1] + box1[:, 3]\n",
    "    box2_x1 = box2[:, 0]\n",
    "    box2_y1 = box2[:, 1]\n",
    "    box2_x2 = box2[:, 0] + box2[:, 2]\n",
    "    box2_y2 = box2[:, 1] + box2[:, 3]\n",
    "    x1 = tf.maximum(box1_x1, box2_x1)\n",
    "    y1 = tf.maximum(box1_y1, box2_y1)\n",
    "    x2 = tf.minimum(box1_x2, box2_x2)\n",
    "    y2 = tf.minimum(box1_y2, box2_y2)\n",
    "    intersection = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
    "    area1 = box1[:, 2] * box1[:, 3]\n",
    "    area2 = box2[:, 2] * box2[:, 3]\n",
    "    union = area1 + area2 - intersection\n",
    "    return tf.math.divide_no_nan(intersection, union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoUMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='iou_metric', **kwargs):\n",
    "        super(IoUMetric, self).__init__(name=name, **kwargs)\n",
    "        self.total_iou = self.add_weight(name='total_iou', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, (-1, 4))\n",
    "        y_pred = tf.reshape(y_pred, (-1, 4))\n",
    "        ious = compute_iou_tf(y_true, y_pred)\n",
    "        self.total_iou.assign_add(tf.reduce_sum(ious))\n",
    "        self.count.assign_add(tf.cast(tf.size(ious), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total_iou, self.count)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total_iou.assign(0.0)\n",
    "        self.count.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbconv_block(x, filters, kernel_size=(3, 3), strides=(1, 1), expand_ratio=4):\n",
    "    input_tensor = x\n",
    "    in_channels = x.shape[-1]\n",
    "    x = layers.Conv2D(in_channels * expand_ratio, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if strides == (1, 1) and in_channels == filters:\n",
    "        x = layers.Add()([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "def simple_efficient_model(input_shape=(64, 64, 3), num_classes=80):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = mbconv_block(x, 64, strides=(1, 1), expand_ratio=2)\n",
    "    x = mbconv_block(x, 64, strides=(2, 2), expand_ratio=2)\n",
    "    x = mbconv_block(x, 128, strides=(1, 1), expand_ratio=4)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    cls = layers.Dense(128, activation='relu')(x)\n",
    "    cls = layers.Dropout(0.4)(cls)\n",
    "    class_output = layers.Dense(num_classes, activation='softmax', name=\"class_output\")(cls)\n",
    "    bbox = layers.Dense(64, activation='relu')(x)\n",
    "    bbox_output = layers.Dense(4, activation='sigmoid', name=\"bbox_output\")(bbox)\n",
    "    model = models.Model(inputs, {\"class_output\": class_output, \"bbox_output\": bbox_output})\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss={\n",
    "            \"class_output\": \"categorical_crossentropy\",\n",
    "            \"bbox_output\": Huber()\n",
    "        },\n",
    "        metrics={\n",
    "            \"class_output\": \"accuracy\",\n",
    "            \"bbox_output\": Huber()\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_efficient_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = paths[\"Trained_model\"],\n",
    "    monitor = 'bbox_output_huber_loss',\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    mode = 'min',\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 23:30:03.138427: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 211524 of 2580003\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs = 2,\n",
    "    validation_data = val_dataset,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
