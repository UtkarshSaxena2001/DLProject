{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7555b4b9",
   "metadata": {},
   "source": [
    "Feature extraction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b3707",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c345c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json as js\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9124b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(x, filters):\n",
    "    branch1x1 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3x3 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3x3 = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(branch3x3)\n",
    "    branch5x5 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch5x5 = layers.Conv2D(filters, (5, 5), padding='same', activation='relu')(branch5x5)\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(branch_pool)\n",
    "    x = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def mbconv_block(x, filters, kernel_size, strides=(1, 1), expand_ratio=6):\n",
    "    input_tensor = x\n",
    "    in_channels = x.shape[-1]\n",
    "    x = layers.Conv2D(in_channels * expand_ratio, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if strides == (1, 1) and in_channels == filters:\n",
    "        x = layers.add([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "def efficientnet_encoder(input_shape=(128, 128, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = mbconv_block(x, 64, (3, 3), strides=(1, 1), expand_ratio=1)\n",
    "    x = mbconv_block(x, 128, (3, 3), strides=(2, 2), expand_ratio=6)\n",
    "    x = mbconv_block(x, 128, (3, 3), strides=(1, 1), expand_ratio=6)\n",
    "    x = mbconv_block(x, 256, (3, 3), strides=(2, 2), expand_ratio=6)\n",
    "    x = mbconv_block(x, 256, (3, 3), strides=(1, 1), expand_ratio=6)\n",
    "    return models.Model(inputs, x)\n",
    "\n",
    "def detection_decoder(input_tensor, num_classes):\n",
    "    x = inception_block(input_tensor, 32)\n",
    "    x = inception_block(x, 64)\n",
    "    x = inception_block(x, 128)\n",
    "    x = inception_block(x, 256)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    bbox_output = layers.Dense(4, activation='sigmoid', name='bbox')(x)\n",
    "    class_output = layers.Dense(num_classes, activation='softmax', name='class')(x)\n",
    "    return bbox_output, class_output\n",
    "\n",
    "def custom_detection_model(input_shape=(128, 128, 3), num_classes=80):\n",
    "    encoder = efficientnet_encoder(input_shape)\n",
    "    x = encoder.output\n",
    "    bbox_output, class_output = detection_decoder(x, num_classes)\n",
    "    model = models.Model(inputs=encoder.input, outputs=[bbox_output, class_output])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss={\n",
    "            'bbox': 'mean_squared_error',\n",
    "            'class': 'categorical_crossentropy',\n",
    "        },\n",
    "        metrics={\n",
    "            'bbox': 'mse',\n",
    "            'class': 'accuracy',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd38f73",
   "metadata": {},
   "source": [
    "Importing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ff73b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_CNN.json\",'r') as file:\n",
    "    paths = js.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96b8d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = paths[\"Preprocessed_Train_json\"] \n",
    "with open(json_path, \"r\") as f:\n",
    "    annotations = js.load(f)\n",
    "category_list = [item[\"category_id\"] for item in annotations]\n",
    "category_list = set(category_list)\n",
    "label2idx = {name: idx for idx, name in enumerate(category_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5357010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = paths[\"Train_resized\"]\n",
    "json_path = paths[\"Preprocessed_Train_json\"] \n",
    "IMG_SIZE = 128 \n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    annotations = js.load(f)\n",
    "\n",
    "annotation_dict = {\n",
    "    item[\"img_id\"]: (item[\"category_id\"], item[\"bbox\"]) for item in annotations\n",
    "} \n",
    "\n",
    "def convert_bbox(bbox):\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_center = x_min + width / 2\n",
    "    y_center = y_min + height / 2\n",
    "\n",
    "    return [\n",
    "        x_center / IMG_SIZE,\n",
    "        y_center / IMG_SIZE,\n",
    "        width / IMG_SIZE,\n",
    "        height / IMG_SIZE,\n",
    "    ]\n",
    "\n",
    "def load_image_and_label(image_path):\n",
    "    filename = tf.strings.split(image_path, os.sep)[-1]\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "\n",
    "    def get_label_and_bbox(f):\n",
    "        f_decoded = f.numpy().decode(\"utf-8\")\n",
    "        category_id, bbox = annotation_dict[f_decoded]\n",
    "        label_idx = label2idx[category_id]\n",
    "        bbox = convert_bbox(bbox)\n",
    "        return np.int32(label_idx), np.array(bbox, dtype=np.float32)\n",
    "\n",
    "    label, bbox = tf.py_function(\n",
    "        func=get_label_and_bbox,\n",
    "        inp=[filename],\n",
    "        Tout=(tf.int32, tf.float32)\n",
    "    )\n",
    "\n",
    "    label.set_shape([])      \n",
    "    bbox.set_shape([4])      \n",
    "    one_hot_label = tf.one_hot(label, depth=80)\n",
    "    return image, {\"bbox\": bbox, \"class\": one_hot_label}\n",
    "\n",
    "def create_dataset(image_dir, batch_size=32, shuffle=True):\n",
    "    image_paths = tf.data.Dataset.list_files(os.path.join(image_dir, \"*.jpg\"), shuffle=shuffle)\n",
    "    dataset = image_paths.map(load_image_and_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(500)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41bdf60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - bbox_loss: 0.1031 - bbox_mse: 0.1031 - class_accuracy: 0.0529 - class_loss: 4.7317 - loss: 4.8349\n",
      "Epoch 1: bbox_mse improved from inf to 0.09069, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 110ms/step - bbox_loss: 0.1031 - bbox_mse: 0.1031 - class_accuracy: 0.0529 - class_loss: 4.7316 - loss: 4.8347\n",
      "Epoch 2/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - bbox_loss: 0.0685 - bbox_mse: 0.0685 - class_accuracy: 0.1494 - class_loss: 3.7617 - loss: 3.8302\n",
      "Epoch 2: bbox_mse improved from 0.09069 to 0.06431, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 109ms/step - bbox_loss: 0.0685 - bbox_mse: 0.0685 - class_accuracy: 0.1494 - class_loss: 3.7616 - loss: 3.8302\n",
      "Epoch 3/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - bbox_loss: 0.0535 - bbox_mse: 0.0535 - class_accuracy: 0.2418 - class_loss: 3.1679 - loss: 3.2214\n",
      "Epoch 3: bbox_mse improved from 0.06431 to 0.05089, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 108ms/step - bbox_loss: 0.0535 - bbox_mse: 0.0535 - class_accuracy: 0.2418 - class_loss: 3.1679 - loss: 3.2214\n",
      "Epoch 4/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0455 - bbox_mse: 0.0455 - class_accuracy: 0.3354 - class_loss: 2.6662 - loss: 2.7117\n",
      "Epoch 4: bbox_mse improved from 0.05089 to 0.04498, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 107ms/step - bbox_loss: 0.0455 - bbox_mse: 0.0455 - class_accuracy: 0.3354 - class_loss: 2.6662 - loss: 2.7117\n",
      "Epoch 5/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - bbox_loss: 0.0440 - bbox_mse: 0.0440 - class_accuracy: 0.3902 - class_loss: 2.3578 - loss: 2.4018\n",
      "Epoch 5: bbox_mse improved from 0.04498 to 0.04383, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 111ms/step - bbox_loss: 0.0440 - bbox_mse: 0.0440 - class_accuracy: 0.3902 - class_loss: 2.3578 - loss: 2.4018\n",
      "Epoch 6/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - bbox_loss: 0.0439 - bbox_mse: 0.0439 - class_accuracy: 0.4517 - class_loss: 2.0768 - loss: 2.1207\n",
      "Epoch 6: bbox_mse improved from 0.04383 to 0.04352, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0439 - bbox_mse: 0.0439 - class_accuracy: 0.4517 - class_loss: 2.0768 - loss: 2.1207\n",
      "Epoch 7/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0431 - bbox_mse: 0.0431 - class_accuracy: 0.5008 - class_loss: 1.8596 - loss: 1.9028\n",
      "Epoch 7: bbox_mse improved from 0.04352 to 0.04325, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 107ms/step - bbox_loss: 0.0431 - bbox_mse: 0.0431 - class_accuracy: 0.5008 - class_loss: 1.8596 - loss: 1.9028\n",
      "Epoch 8/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - bbox_loss: 0.0430 - bbox_mse: 0.0430 - class_accuracy: 0.5477 - class_loss: 1.6604 - loss: 1.7034\n",
      "Epoch 8: bbox_mse improved from 0.04325 to 0.04310, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 138ms/step - bbox_loss: 0.0430 - bbox_mse: 0.0430 - class_accuracy: 0.5477 - class_loss: 1.6604 - loss: 1.7034\n",
      "Epoch 9/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - bbox_loss: 0.0428 - bbox_mse: 0.0428 - class_accuracy: 0.5908 - class_loss: 1.4806 - loss: 1.5234\n",
      "Epoch 9: bbox_mse improved from 0.04310 to 0.04297, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 283ms/step - bbox_loss: 0.0428 - bbox_mse: 0.0428 - class_accuracy: 0.5908 - class_loss: 1.4806 - loss: 1.5235\n",
      "Epoch 10/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - bbox_loss: 0.0429 - bbox_mse: 0.0429 - class_accuracy: 0.6262 - class_loss: 1.3276 - loss: 1.3705\n",
      "Epoch 10: bbox_mse improved from 0.04297 to 0.04296, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 112ms/step - bbox_loss: 0.0429 - bbox_mse: 0.0429 - class_accuracy: 0.6262 - class_loss: 1.3277 - loss: 1.3706\n",
      "Epoch 11/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - bbox_loss: 0.0427 - bbox_mse: 0.0427 - class_accuracy: 0.6647 - class_loss: 1.1768 - loss: 1.2195\n",
      "Epoch 11: bbox_mse improved from 0.04296 to 0.04283, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 108ms/step - bbox_loss: 0.0427 - bbox_mse: 0.0427 - class_accuracy: 0.6647 - class_loss: 1.1768 - loss: 1.2195\n",
      "Epoch 12/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - bbox_loss: 0.0426 - bbox_mse: 0.0426 - class_accuracy: 0.6964 - class_loss: 1.0487 - loss: 1.0913\n",
      "Epoch 12: bbox_mse improved from 0.04283 to 0.04277, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 108ms/step - bbox_loss: 0.0426 - bbox_mse: 0.0426 - class_accuracy: 0.6964 - class_loss: 1.0487 - loss: 1.0913\n",
      "Epoch 13/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0427 - bbox_mse: 0.0427 - class_accuracy: 0.7286 - class_loss: 0.9231 - loss: 0.9658\n",
      "Epoch 13: bbox_mse improved from 0.04277 to 0.04270, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 107ms/step - bbox_loss: 0.0427 - bbox_mse: 0.0427 - class_accuracy: 0.7286 - class_loss: 0.9232 - loss: 0.9659\n",
      "Epoch 14/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0426 - bbox_mse: 0.0426 - class_accuracy: 0.7577 - class_loss: 0.8161 - loss: 0.8587\n",
      "Epoch 14: bbox_mse improved from 0.04270 to 0.04259, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 107ms/step - bbox_loss: 0.0426 - bbox_mse: 0.0426 - class_accuracy: 0.7577 - class_loss: 0.8161 - loss: 0.8587\n",
      "Epoch 15/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0424 - bbox_mse: 0.0424 - class_accuracy: 0.7875 - class_loss: 0.7118 - loss: 0.7542\n",
      "Epoch 15: bbox_mse improved from 0.04259 to 0.04258, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 107ms/step - bbox_loss: 0.0424 - bbox_mse: 0.0424 - class_accuracy: 0.7875 - class_loss: 0.7119 - loss: 0.7543\n",
      "Epoch 16/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0424 - bbox_mse: 0.0424 - class_accuracy: 0.8038 - class_loss: 0.6394 - loss: 0.6818\n",
      "Epoch 16: bbox_mse improved from 0.04258 to 0.04249, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0424 - bbox_mse: 0.0424 - class_accuracy: 0.8038 - class_loss: 0.6394 - loss: 0.6818\n",
      "Epoch 17/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0423 - bbox_mse: 0.0423 - class_accuracy: 0.8242 - class_loss: 0.5748 - loss: 0.6172\n",
      "Epoch 17: bbox_mse improved from 0.04249 to 0.04244, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0423 - bbox_mse: 0.0423 - class_accuracy: 0.8242 - class_loss: 0.5748 - loss: 0.6172\n",
      "Epoch 18/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0422 - bbox_mse: 0.0422 - class_accuracy: 0.8461 - class_loss: 0.5047 - loss: 0.5469\n",
      "Epoch 18: bbox_mse improved from 0.04244 to 0.04236, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0422 - bbox_mse: 0.0422 - class_accuracy: 0.8461 - class_loss: 0.5047 - loss: 0.5469\n",
      "Epoch 19/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0423 - bbox_mse: 0.0423 - class_accuracy: 0.8531 - class_loss: 0.4730 - loss: 0.5152\n",
      "Epoch 19: bbox_mse improved from 0.04236 to 0.04228, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0423 - bbox_mse: 0.0423 - class_accuracy: 0.8531 - class_loss: 0.4730 - loss: 0.5152\n",
      "Epoch 20/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0423 - bbox_mse: 0.0423 - class_accuracy: 0.8654 - class_loss: 0.4327 - loss: 0.4751\n",
      "Epoch 20: bbox_mse improved from 0.04228 to 0.04218, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0423 - bbox_mse: 0.0423 - class_accuracy: 0.8653 - class_loss: 0.4328 - loss: 0.4751\n",
      "Epoch 21/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0421 - bbox_mse: 0.0421 - class_accuracy: 0.8813 - class_loss: 0.3787 - loss: 0.4209\n",
      "Epoch 21: bbox_mse improved from 0.04218 to 0.04201, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0421 - bbox_mse: 0.0421 - class_accuracy: 0.8813 - class_loss: 0.3788 - loss: 0.4209\n",
      "Epoch 22/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0419 - bbox_mse: 0.0419 - class_accuracy: 0.8903 - class_loss: 0.3557 - loss: 0.3976\n",
      "Epoch 22: bbox_mse improved from 0.04201 to 0.04199, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0419 - bbox_mse: 0.0419 - class_accuracy: 0.8903 - class_loss: 0.3557 - loss: 0.3976\n",
      "Epoch 23/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0419 - bbox_mse: 0.0419 - class_accuracy: 0.8919 - class_loss: 0.3457 - loss: 0.3877\n",
      "Epoch 23: bbox_mse improved from 0.04199 to 0.04187, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0419 - bbox_mse: 0.0419 - class_accuracy: 0.8919 - class_loss: 0.3457 - loss: 0.3877\n",
      "Epoch 24/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0418 - bbox_mse: 0.0418 - class_accuracy: 0.8966 - class_loss: 0.3256 - loss: 0.3674\n",
      "Epoch 24: bbox_mse improved from 0.04187 to 0.04178, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 107ms/step - bbox_loss: 0.0418 - bbox_mse: 0.0418 - class_accuracy: 0.8966 - class_loss: 0.3256 - loss: 0.3674\n",
      "Epoch 25/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - bbox_loss: 0.0415 - bbox_mse: 0.0415 - class_accuracy: 0.9044 - class_loss: 0.3030 - loss: 0.3445\n",
      "Epoch 25: bbox_mse improved from 0.04178 to 0.04165, saving model to /home/utkarsh/Desktop/Sem-2/Deep Learning/2024PGCSDS14_Utkarsh Saxena_DeepLearning/DLProject/best_model_bbox.keras\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 107ms/step - bbox_loss: 0.0415 - bbox_mse: 0.0415 - class_accuracy: 0.9044 - class_loss: 0.3030 - loss: 0.3446\n"
     ]
    }
   ],
   "source": [
    "model = custom_detection_model()\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=paths[\"Trained_model\"],            \n",
    "    monitor='bbox_mse',                 \n",
    "    save_best_only=True,                \n",
    "    save_weights_only=False,              \n",
    "    mode='min',                          \n",
    "    verbose=1                           \n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=25,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb10bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
