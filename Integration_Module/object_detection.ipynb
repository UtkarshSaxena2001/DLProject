{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7555b4b9",
   "metadata": {},
   "source": [
    "Feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9124b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.applications import InceptionV3,EfficientNetB0 #type: ignore\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def inception_block(x, filters):\n",
    "    branch1x1 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3x3 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3x3 = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(branch3x3)\n",
    "    branch5x5 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch5x5 = layers.Conv2D(filters, (5, 5), padding='same', activation='relu')(branch5x5)\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(branch_pool)\n",
    "    x = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def mbconv_block(x, filters, kernel_size, strides=(1, 1), expand_ratio=6):\n",
    "    input_tensor = x\n",
    "    in_channels = x.shape[-1]\n",
    "    x = layers.Conv2D(in_channels * expand_ratio, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if strides == (1, 1) and in_channels == filters:\n",
    "        x = layers.add([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "def efficientnet_encoder(input_shape=(224, 224, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = mbconv_block(x, 64, (3, 3), strides=(1, 1), expand_ratio=1)\n",
    "    x = mbconv_block(x, 128, (3, 3), strides=(2, 2), expand_ratio=6)\n",
    "    x = mbconv_block(x, 128, (3, 3), strides=(1, 1), expand_ratio=6)\n",
    "    x = mbconv_block(x, 256, (3, 3), strides=(2, 2), expand_ratio=6)\n",
    "    x = mbconv_block(x, 256, (3, 3), strides=(1, 1), expand_ratio=6)\n",
    "    x = mbconv_block(x, 512, (3, 3), strides=(2, 2), expand_ratio=6)\n",
    "    x = mbconv_block(x, 512, (3, 3), strides=(1, 1), expand_ratio=6)\n",
    "    x = mbconv_block(x, 1024, (3, 3), strides=(2, 2), expand_ratio=6)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return models.Model(inputs, x)\n",
    "\n",
    "def inception_decoder(input_tensor, num_classes=3):\n",
    "    x = input_tensor\n",
    "    x = inception_block(x, 32)\n",
    "    x = inception_block(x, 64)  \n",
    "    x = inception_block(x, 128)\n",
    "    x = inception_block(x, 256)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)  \n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(input_tensor, x), \n",
    "\n",
    "def custom_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    encoder = efficientnet_encoder(input_shape)  \n",
    "    x = encoder.output  \n",
    "    print(\"Encoder output shape:\", x.shape)\n",
    "    x = layers.Reshape((1, 1, 1024))(x)  \n",
    "    x = layers.Conv2D(1024, (1, 1), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    decoder = inception_decoder(input_tensor=x, num_classes=num_classes)\n",
    "    outputs = decoder(x)  \n",
    "    model = models.Model(inputs=encoder.input, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/resized_train2017\"\n",
    "val_dir = \"data/resized_val2017\"\n",
    "json_path = \"your_annotations_file.json\"  # Replace with actual path\n",
    "\n",
    "# Load the annotation JSON\n",
    "with open(json_path, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Build lookup dict: {filename: (label, [x, y, w, h])}\n",
    "annotation_dict = {\n",
    "    item[\"filename\"]: (item[\"class_id\"], item[\"bbox\"]) for item in annotations\n",
    "}\n",
    "\n",
    "# Function to load image, label and bbox\n",
    "def load_image_and_label(image_path):\n",
    "    filename = tf.strings.split(image_path, os.sep)[-1]\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # normalize [0, 1]\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "\n",
    "    label_and_bbox = tf.py_function(\n",
    "        func=lambda f: annotation_dict[f.numpy().decode()],\n",
    "        inp=[filename],\n",
    "        Tout=(tf.int32, tf.float32)\n",
    "    )\n",
    "    label, bbox = label_and_bbox\n",
    "    bbox.set_shape([4])\n",
    "    return image, (label, bbox)\n",
    "\n",
    "# Create datasets\n",
    "def create_dataset(image_dir, batch_size=32, shuffle=True):\n",
    "    image_paths = tf.data.Dataset.list_files(os.path.join(image_dir, \"*.jpg\"), shuffle=shuffle)\n",
    "    dataset = image_paths.map(load_image_and_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(500)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(train_dir)\n",
    "val_ds = create_dataset(val_dir, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22c09404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    cnn_model = custom_model()  \n",
    "    feature_model = tf.keras.Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(index=-4).output)  \n",
    "    return feature_model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7f1347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionGenerator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(CaptionGenerator, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features):\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(features, 1), x], axis=-2)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "519e8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_decoder(vocab_size, max_caption_length, embedding_dim=256, feature_dim=512, units=512):\n",
    "    image_input = layers.Input(shape=(feature_dim,))\n",
    "    image_emb = layers.Dense(units, activation='relu')(image_input)\n",
    "    image_emb = layers.RepeatVector(max_caption_length)(image_emb) \n",
    "    caption_input = layers.Input(shape=(max_caption_length,))\n",
    "    caption_emb = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(caption_input)\n",
    "    x = layers.concatenate([image_emb, caption_emb])\n",
    "    x = layers.LSTM(units, return_sequences=True)(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(x)\n",
    "    model = models.Model(inputs=[image_input, caption_input], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85d740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = layers.Input(shape=(128, 128, 3))\n",
    "caption_input = layers.Input(shape=(158,))\n",
    "cnn_encoder = custom_cnn_encoder(input_shape=(128, 128, 3))\n",
    "image_features = cnn_encoder(image_input)\n",
    "decoder = rnn_decoder(10000, 158)\n",
    "caption_output = decoder([image_features, caption_input])\n",
    "full_model = models.Model(inputs=[image_input, caption_input], outputs=caption_output)\n",
    "full_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21eff477",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'images/' \n",
    "CAPTIONS_FILE = 'captions.txt'  \n",
    "MAX_LENGTH = 30\n",
    "VOCAB_SIZE = 10000  \n",
    "\n",
    "def load_captions_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        captions_dict = json.load(f)\n",
    "\n",
    "    for img in captions_dict:\n",
    "        img['output'] = [f\"<start> {cap} <end>\" for cap in img['output']]\n",
    "    return captions_dict\n",
    "\n",
    "def build_text_vectorizer(captions_dict, vocab_size=VOCAB_SIZE, max_length=MAX_LENGTH):\n",
    "    all_captions = []\n",
    "    for cap_list in captions_dict.values():\n",
    "        all_captions.extend(cap_list)\n",
    "\n",
    "    vectorizer = layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,\n",
    "        standardize='lower_and_strip_punctuation'\n",
    "    )\n",
    "    vectorizer.adapt(tf.data.Dataset.from_tensor_slices(all_captions).batch(64))\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2b58274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, target_size=(128, 128)):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    return np.array(img) / 255.0\n",
    "\n",
    "def create_dataset(captions_dict, vectorizer, max_length=30):\n",
    "    img_tensors, cap_inputs, cap_targets, cap_all = [], [], [], []\n",
    "    for img_name, captions in tqdm(captions_dict.items()):\n",
    "        img_path = os.path.join(IMG_DIR, img_name)\n",
    "        img_array = load_image(img_path)\n",
    "        for cap in captions:\n",
    "            seq = vectorizer([cap])[0].numpy()\n",
    "            input_seq = seq[:-1]\n",
    "            target_seq = seq[1:]\n",
    "            input_seq = np.pad(input_seq, (0, max_length - 1 - len(input_seq)), constant_values=0)\n",
    "            target_seq = np.pad(target_seq, (0, max_length - 1 - len(target_seq)), constant_values=0)\n",
    "            combined_seq = np.concatenate([input_seq, target_seq])\n",
    "            img_tensors.append(img_array)\n",
    "            cap_inputs.append(input_seq)\n",
    "            cap_targets.append(target_seq)\n",
    "            cap_all.append(combined_seq)\n",
    "\n",
    "    return (\n",
    "        np.array(img_tensors),\n",
    "        np.array(cap_inputs, dtype=np.int32),\n",
    "        np.expand_dims(np.array(cap_targets, dtype=np.int32), -1),\n",
    "        np.array(cap_all, dtype=np.int32)  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee5246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(images, cap_inputs, cap_targets, vectorizer, batch_size=64, epochs=20):\n",
    "    cnn_features = extract_features(images)\n",
    "    embedding_dim = 256\n",
    "    units = 512\n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "    rnn_model = CaptionGenerator(embedding_dim, units, vocab_size)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    rnn_model.fit(\n",
    "        [cap_inputs, cnn_features],\n",
    "        cap_targets,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdf60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_pipeline(images, cap_inputs, cap_targets, vectorizer, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
