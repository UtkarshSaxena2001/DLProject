{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a2776c",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75496f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 21:57:39.918122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 21:57:39.929205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-23 21:57:39.932470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-23 21:57:39.940812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json as js\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from keras import layers, optimizers\n",
    "from keras.models import Model #type: ignore\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, Embedding #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e55bc2",
   "metadata": {},
   "source": [
    "loading paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e612b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Config_RNN.json','r') as file:\n",
    "    paths = js.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578de471",
   "metadata": {},
   "source": [
    "loading padded captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dcb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paths[\"Padded_preprocessed_data\"],'r') as f:\n",
    "    padded_captions = js.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1be88",
   "metadata": {},
   "source": [
    "Build vocabulary and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9891f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word for cap in padded_captions for word in cap)\n",
    "word2idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "one_hot_captions = []\n",
    "\n",
    "for cap in padded_captions:\n",
    "    encoded = []\n",
    "\n",
    "    for word in cap:\n",
    "        one_hot = [0] * vocab_size\n",
    "        one_hot[word2idx[word]] = 1\n",
    "        encoded.append(one_hot)\n",
    "\n",
    "    one_hot_captions.append(encoded)\n",
    "    \n",
    "one_hot_captions = np.array(one_hot_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b16f2",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0608f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=padded_captions, vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v_captions = []\n",
    "\n",
    "for cap in padded_captions:\n",
    "    encoded = [model.wv[word] for word in cap]\n",
    "    w2v_captions.append(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875de4f",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56788c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745425974.514704   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.561864   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.571823   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.578587   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.582272   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.587738   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.747685   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.751474   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745425974.754369   23644 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-23 22:02:54.756180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3579 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "input_seq_len = 158\n",
    "output_seq_len = 30\n",
    "vector_dim = 158\n",
    "hidden_units = 256 \n",
    "encoder_inputs = Input(shape=(input_seq_len, vector_dim))\n",
    "encoder = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "decoder_inputs = tf.keras.layers.RepeatVector(output_seq_len)(state_h)\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "decoder_dense = TimeDistributed(Dense(vector_dim))\n",
    "final_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model(inputs=encoder_inputs, outputs=final_outputs)\n",
    "model.compile(optimizer='adam', loss='mse') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da2938",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f509ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.layers import TextVectorization\n",
    "# ========== SETUP ==========\n",
    "vocab_size = 10000\n",
    "seq_length = 30\n",
    "embedding_dim = 100\n",
    "units = 512\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "# Sample input: padded_captions = [{'output': 'a man riding a bike'}, ...]\n",
    "# Sample input: w2v_captions = [np.array([...]), np.array([...]), ...]\n",
    "# ========== TEXT VECTORIZATION ==========\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "all_captions = [\"<start> \" + item['output'] + \" <end>\" for item in padded_captions]\n",
    "vectorizer.adapt(all_captions)\n",
    "\n",
    "input_tensor = np.array([np.mean(cap, axis=0) for cap in w2v_captions])\n",
    "target_tensor = vectorizer(tf.constant(all_captions)).numpy()\n",
    "\n",
    "# ========== DATA SPLIT ==========\n",
    "split_index = int(0.8 * len(input_tensor))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (input_tensor[:split_index], target_tensor[:split_index])\n",
    ").shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (input_tensor[split_index:], target_tensor[split_index:])\n",
    ").batch(BATCH_SIZE)\n",
    "\n",
    "# ========== MODEL SETUP ==========\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the Decoder\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = layers.GRU(self.units,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = layers.Dense(self.units)\n",
    "        self.fc2 = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        x = self.embedding(x)\n",
    "        features = tf.expand_dims(features, 1)\n",
    "        features = tf.tile(features, [1, tf.shape(x)[1], 1])  # Match sequence length\n",
    "        x = tf.concat([features, x], axis=-1)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        x = self.fc1(output)\n",
    "        x = self.fc2(x)\n",
    "        return x, state, None\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.not_equal(real, 0)\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target, decoder, optimizer):\n",
    "    loss = 0\n",
    "    batch_size = tf.shape(img_tensor)[0]\n",
    "    hidden = decoder.reset_state(batch_size)\n",
    "    dec_input = target[:, :-1]\n",
    "    real = target[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _, _ = decoder(dec_input, img_tensor, hidden)\n",
    "        loss = loss_function(real, predictions)\n",
    "\n",
    "    trainable_variables = decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(img_tensor, target, decoder):\n",
    "    loss = 0\n",
    "    batch_size = tf.shape(img_tensor)[0]\n",
    "    hidden = decoder.reset_state(batch_size)\n",
    "    dec_input = target[:, :-1]\n",
    "    real = target[:, 1:]\n",
    "\n",
    "    predictions, _, _ = decoder(dec_input, img_tensor, hidden)\n",
    "    loss = loss_function(real, predictions)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_model(train_dataset, val_dataset, decoder, optimizer, epochs, save_path):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        for img_tensor, target in tqdm(train_dataset, desc=\"Training\"):\n",
    "            batch_loss = train_step(img_tensor, target, decoder, optimizer)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "        print(f\"Training Loss: {total_loss/len(train_dataset):.4f}\")\n",
    "\n",
    "        total_val_loss = 0\n",
    "        for img_tensor, target in tqdm(val_dataset, desc=\"Validating\"):\n",
    "            batch_val_loss = validation_step(img_tensor, target, decoder)\n",
    "            total_val_loss += batch_val_loss\n",
    "\n",
    "        val_loss = total_val_loss / len(val_dataset)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"Validation loss improved. Saving model.\")\n",
    "            decoder.save_weights(save_path)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            print(\"Validation loss did not improve.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
